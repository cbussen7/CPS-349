# -*- coding: utf-8 -*-
"""project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I9zi_6hq5NWEGmJ9-2Vepti8SSyzaXFK
"""

# import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from wordcloud import WordCloud

# load in the datasets
wordData1 = pd.read_csv("google-books-common-words.csv")
wordData2 = pd.read_csv("kaggle_freq.csv")
wordData3 = pd.read_csv("wordFrequency.csv")

# use dataset created in other file - already sorted
combinedData = pd.read_csv("combinedWordFrequencies.csv")

# words in 1st dataset are all caps - change to lower case
wordData1["word"] = wordData1["word"].str.lower()

# ** EXPERIMENT 1 **

# display frequency table and bar graph of 25 most common words and their frequencies
print(combinedData.head(25), "\n")
print(combinedData.info(), "\n")
freqPlot = combinedData.head(25).plot.bar(x='word', y='frequency', rot=90, title='Frequencies of most common words', legend=False)
freqPlot.set(xlabel="Word", ylabel="Frequency")
print(freqPlot, "\n")

# use wordcloud package to create a wordcloud of words and their frequencies
wordFrequencies = dict(zip(combinedData['word'], combinedData['frequency']))
cloud = WordCloud(background_color='white').generate_from_frequencies(wordFrequencies)

plt.imshow(cloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# create a column for relative frequency and create new histogram of relative frequency
# relative freq = frequency / total frequency
combinedData['relativeFrequency'] = combinedData['frequency'] / combinedData['frequency'].sum() 

# display top 5 relative frequencies
print("Relative frequencies of top 5 words from trimmed dataset:\n", combinedData['relativeFrequency'].head(), "\n")

# ** EXPERIMENT 2 **

# add column for length to each individual dataset (and combined to use for later)
wordData1['length'] = wordData1['word'].str.len()
wordData2['length'] = wordData2['word'].str.len()
wordData3['length'] = wordData3['word'].str.len()
combinedData['length'] = combinedData['word'].str.len()

# sorted = wordData1.sort_values('length', ascending=False)
# print(sorted.head(10))
# print(wordData1['length'].mode())

# create and print histograms for length of words for each dataset

# originally just used these print statements, switched to code below so I could add labels
# print(wordData1.hist(column="length", bins=20, range=[0,25]), "\n")
# print(wordData2.hist(column="length",bins=20, range=[0,25]), "\n")
# print(wordData3.hist(column="length",bins=15, range=[0,25]), "\n")
wordData1.hist(column="length", bins=20, range=[0,25])
plt.xlabel("Word Length")
plt.ylabel("Frequency")
plt.title("Distribution of Word Length for Dataset 1")
plt.show

wordData2.hist(column="length",bins=20, range=[0,25])
plt.xlabel("Word Length")
plt.ylabel("Frequency")
plt.title("Distribution of Word Length for Dataset 2")
plt.show

wordData3.hist(column="length",bins=20, range=[0,25])
plt.xlabel("Word Length")
plt.ylabel("Frequency")
plt.title("Distribution of Word Length for Dataset 3")
plt.show

# calculate and print the average word length of each dataset
print("Mean word length for the first dataset:", wordData1['length'].mean())
print("Mean word length for the second dataset:", wordData2['length'].mean())
print("Mean word length for the third dataset:", wordData3['length'].mean(), "\n")

# sort each dataset by frequency and get average length of top 25 entries of each
sorted1 = wordData1.sort_values('frequency', ascending=False)
sorted2 = wordData2.sort_values('frequency', ascending=False)
sorted3 = wordData3.sort_values('frequency', ascending=False)

print("Mean length of top 25 words in the first dataset:", sorted1['length'].head(25).mean())
print("Mean length of top 25 words in the first dataset:", sorted2['length'].head(25).mean())
print("Mean length of top 25 words in the first dataset:", sorted3['length'].head(25).mean())

# ** EXPERIMENT 3 **

# eliminate any words less than 6 letters long from the dataset and save to new trimmed dataframe
totalEntries = combinedData['word'].count()
trimmedLengthSet = combinedData.drop(combinedData[combinedData['length'] < 6].index)
# use count method to find how many entries were discarded
difference = totalEntries - trimmedLengthSet['word'].count()
print("Trimming the dataset based on length gets rid of", difference, "entries\n")
print(trimmedLengthSet.head(10), "\n")

# sort trimmed dataset based on descending frequency
sortedTrimmed = trimmedLengthSet.sort_values('frequency', ascending=False)

# plot a bar graph of the frequencies of the new top 25 words
trimFreqPlot = sortedTrimmed.head(25).plot.bar(x='word', y='frequency', rot=90, title="Most Common 6+ Letter Words", legend=False)
trimFreqPlot.set(xlabel='Word', ylabel='Frequency')
print(trimFreqPlot, "\n")

# create a column for relative frequency and create new histogram of relative frequency
# relative freq = frequency / total frequency
sortedTrimmed['relativeFrequency'] = sortedTrimmed['frequency'] / sortedTrimmed['frequency'].sum() 

# display top 5 relative frequencies
print("Relative frequencies of top 5 words from trimmed dataset:\n", sortedTrimmed['relativeFrequency'].head(), "\n")